{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataModule import MidiDataModule\n",
    "from models.LSTMDiscriminator import LSTMDiscriminator\n",
    "from models.LSTMGenerator import LSTMGenerator\n",
    "from pytorch_lightning import LightningModule,Trainer\n",
    "from torch.functional import F\n",
    "from torch.optim import SGD\n",
    "import torch,logging\n",
    "import DownloadData\n",
    "import TransformData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"running.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 557/4174 [00:57<06:15,  9.64it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/asimpoptani/LMIP-full/src/LSTMNotebook.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/asimpoptani/LMIP-full/src/LSTMNotebook.ipynb#ch0000007?line=0'>1</a>\u001b[0m DownloadData\u001b[39m.\u001b[39mdownloadDatasets(tmp_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../tmp/\u001b[39m\u001b[39m\"\u001b[39m,unprocessed_data_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../data/unprocessed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/asimpoptani/LMIP-full/src/LSTMNotebook.ipynb#ch0000007?line=1'>2</a>\u001b[0m TransformData\u001b[39m.\u001b[39;49mtransformData(unprocessed_data_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../data/unprocessed\u001b[39;49m\u001b[39m'\u001b[39;49m,processed_data_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../data/processed\u001b[39;49m\u001b[39m'\u001b[39;49m,csv_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../data/index.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/LMIP-full/src/TransformData.py:28\u001b[0m, in \u001b[0;36mtransformData\u001b[0;34m(logger, unprocessed_data_dir, processed_data_dir, replication, instruments, csv_file)\u001b[0m\n\u001b[1;32m     18\u001b[0m csv_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(csv_file)\n\u001b[1;32m     20\u001b[0m filter_midi \u001b[39m=\u001b[39m FilterMIDI(\n\u001b[1;32m     21\u001b[0m     folder_output\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(current_path, processed_data_dir),\n\u001b[1;32m     22\u001b[0m     csv_file\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(current_path, csv_file),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     replication\u001b[39m=\u001b[39mreplication,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m filter_midi\u001b[39m.\u001b[39;49mfilter_and_extract_folder(\n\u001b[1;32m     29\u001b[0m     os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(current_path, unprocessed_data_dir)\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/LMIP-full/src/midi_to_csv.py:155\u001b[0m, in \u001b[0;36mFilterMIDI.filter_and_extract_folder\u001b[0;34m(self, midi_folder)\u001b[0m\n\u001b[1;32m    153\u001b[0m midi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_midi(midi_file)\n\u001b[1;32m    154\u001b[0m \u001b[39m# Save the midi file\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m midi\u001b[39m.\u001b[39;49mwrite(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_folder_output, new_midi_file_name))\n\u001b[1;32m    156\u001b[0m \u001b[39m# Append the metadata to the csv file (previous_midi_file_name,new_midi_file_name)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m csv_file\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmidi_file\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mnew_midi_file_name\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/LMIP-full/venv/lib/python3.9/site-packages/pretty_midi/pretty_midi.py:1374\u001b[0m, in \u001b[0;36mPrettyMIDI.write\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     track\u001b[39m.\u001b[39mappend(mido\u001b[39m.\u001b[39mMessage(\n\u001b[1;32m   1370\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnote_on\u001b[39m\u001b[39m'\u001b[39m, time\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_to_tick(note\u001b[39m.\u001b[39mstart),\n\u001b[1;32m   1371\u001b[0m         channel\u001b[39m=\u001b[39mchannel, note\u001b[39m=\u001b[39mnote\u001b[39m.\u001b[39mpitch, velocity\u001b[39m=\u001b[39mnote\u001b[39m.\u001b[39mvelocity))\n\u001b[1;32m   1372\u001b[0m     \u001b[39m# Also need a note-off event (note on with velocity 0)\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m     track\u001b[39m.\u001b[39mappend(mido\u001b[39m.\u001b[39mMessage(\n\u001b[0;32m-> 1374\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnote_on\u001b[39m\u001b[39m'\u001b[39m, time\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_to_tick(note\u001b[39m.\u001b[39;49mend),\n\u001b[1;32m   1375\u001b[0m         channel\u001b[39m=\u001b[39mchannel, note\u001b[39m=\u001b[39mnote\u001b[39m.\u001b[39mpitch, velocity\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[39m# Add all pitch bend events\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[39mfor\u001b[39;00m bend \u001b[39min\u001b[39;00m instrument\u001b[39m.\u001b[39mpitch_bends:\n",
      "File \u001b[0;32m~/LMIP-full/venv/lib/python3.9/site-packages/pretty_midi/pretty_midi.py:999\u001b[0m, in \u001b[0;36mPrettyMIDI.time_to_tick\u001b[0;34m(self, time)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39m\"\"\"Converts from a time in seconds to absolute tick using\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39m``self._tick_scales``.\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    996\u001b[0m \n\u001b[1;32m    997\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[39m# Find the index of the ticktime which is smaller than time\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m tick \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msearchsorted(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__tick_to_time, time, side\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1000\u001b[0m \u001b[39m# If the closest tick was the final tick in self.__tick_to_time...\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[39mif\u001b[39;00m tick \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tick_to_time):\n\u001b[1;32m   1002\u001b[0m     \u001b[39m# start from time at end of __tick_to_time\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/LMIP-full/venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1387\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearchsorted\u001b[39m(a, v, side\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, sorter\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1321\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[39m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1385\u001b[0m \n\u001b[1;32m   1386\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1387\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39msearchsorted\u001b[39;49m\u001b[39m'\u001b[39;49m, v, side\u001b[39m=\u001b[39;49mside, sorter\u001b[39m=\u001b[39;49msorter)\n",
      "File \u001b[0;32m~/LMIP-full/venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/LMIP-full/venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DownloadData.downloadDatasets(tmp_dir=\"../tmp/\",unprocessed_data_dir='../data/unprocessed')\n",
    "TransformData.transformData(unprocessed_data_dir='../data/unprocessed',processed_data_dir='../data/processed',csv_file='../data/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module=MidiDataModule(csv_file=\"../data/index.csv\",path_to_midi=\"../data/processed\",batch_size=1,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup('fit')\n",
    "train_loader=data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLSTM(LightningModule):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "        self.generator=LSTMGenerator()\n",
    "        self.discriminator=LSTMDiscriminator()\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self,random_or_info):\n",
    "        pitch,velocity,duration,step,info=self.generator(random_or_info)\n",
    "        return pitch,velocity,duration,step,info\n",
    "    \n",
    "    def ad_loss(self,y_hat,y):\n",
    "        return F.binary_cross_entropy(y_hat,y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        # 'pitch_tensors', 'velocity_tensors', 'duration_tensors', 'step_tensors'\n",
    "        length_of_batch=len(batch['pitch_tensors'])\n",
    "\n",
    "        if optimizer_idx==0:\n",
    "            \n",
    "            \n",
    "            # Generate random data\n",
    "            random_data=None\n",
    "\n",
    "            generated_data={'pitch_tensors':[],'velocity_tensors':[],'duration_tensors':[],'step_tensors':[]}\n",
    "\n",
    "            # Train the generator\n",
    "            for _ in range(length_of_batch):\n",
    "                pitch,velocity,duration,step,random_data=self.generator(random_data)\n",
    "                generated_data['pitch_tensors'].append(pitch)\n",
    "                generated_data['velocity_tensors'].append(velocity)\n",
    "                generated_data['duration_tensors'].append(duration)\n",
    "                generated_data['step_tensors'].append(step)\n",
    "\n",
    "            # Feed into the discriminator\n",
    "            # pitches,velocities,durations,steps\n",
    "            y_hat=self.discriminator(generated_data['pitch_tensors'],generated_data['velocity_tensors'],generated_data['duration_tensors'],generated_data['step_tensors'])\n",
    "            y=torch.ones(y_hat.shape).type_as(y_hat)\n",
    "            loss_g=self.ad_loss(y_hat,y)\n",
    "            return {'loss':loss_g,'log':{'loss_g':loss_g}}\n",
    "        \n",
    "        elif optimizer_idx==1:\n",
    "            \n",
    "            # Real data\n",
    "            \n",
    "\n",
    "            y_hat=self.discriminator(batch['pitch_tensors'],batch['velocity_tensors'],batch['duration_tensors'],batch['step_tensors'])\n",
    "           \n",
    "            y=torch.ones(y_hat.shape).type_as(y_hat)\n",
    "            real_loss=self.ad_loss(y_hat,y)\n",
    "            \n",
    "            \n",
    "            # Fake data\n",
    "            # Generate fake data\n",
    "            random_data=None\n",
    "\n",
    "            generated_data={'pitch_tensors':[],'velocity_tensors':[],'duration_tensors':[],'step_tensors':[]}\n",
    "\n",
    "            # Train the generator\n",
    "            for _ in range(length_of_batch):\n",
    "                pitch,velocity,duration,step,random_data=self.generator(random_data)\n",
    "                generated_data['pitch_tensors'].append(pitch)\n",
    "                generated_data['velocity_tensors'].append(velocity)\n",
    "                generated_data['duration_tensors'].append(duration)\n",
    "                generated_data['step_tensors'].append(step)\n",
    "            \n",
    "            y_hat=self.discriminator(generated_data['pitch_tensors'],generated_data['velocity_tensors'],generated_data['duration_tensors'],generated_data['step_tensors'])\n",
    "            y=torch.zeros(y_hat.shape).type_as(y_hat)\n",
    "            \n",
    "            fake_loss=self.ad_loss(y_hat,y)\n",
    "\n",
    "            total_loss=real_loss+fake_loss\n",
    "            total_loss=total_loss/2\n",
    "            return {'loss':total_loss,'log':{'loss_d':total_loss,'loss_real':real_loss,'loss_fake':fake_loss}}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        opt_g = SGD(self.generator.parameters(), lr=0.001, momentum=0.9)\n",
    "        opt_d = SGD(self.discriminator.parameters(), lr=0.001, momentum=0.9)\n",
    "        return [opt_g, opt_d], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | generator     | LSTMGenerator     | 2.6 M \n",
      "1 | discriminator | LSTMDiscriminator | 2.9 M \n",
      "----------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "22.085    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a8e06db1554440b6bb95344b8cf76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Generator\n",
      "info torch.Size([500])\n",
      "Training Discriminator\n",
      "info torch.Size([500])\n",
      "info torch.Size([500])\n",
      "Training Generator\n",
      "info torch.Size([500])\n",
      "Training Discriminator\n",
      "info torch.Size([500])\n",
      "info torch.Size([500])\n",
      "Training Generator\n",
      "info torch.Size([500])\n",
      "Training Discriminator\n",
      "info torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "gan_lstm=GANLSTM()\n",
    "Trainer(accelerator=\"gpu\").fit(gan_lstm,train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a044009396592af81d2a127d6c16be8dafb71f8e60cd761b1de69625db2d005c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
